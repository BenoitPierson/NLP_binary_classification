{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP model building with long short term memory LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose : Building a model able to do a classification on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Interesting links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "* https://www.hackdeploy.com/keras-lstm-example-sequence-binary-classification/\n",
    "* https://www.youtube.com/watch?v=A9QVYOBjZdY&ab_channel=TensorFlow\n",
    "* https://www.youtube.com/watch?v=ZMudJXhsUpY&ab_channel=TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train = 10000 # Number of training examples considered\n",
    "nb_valid = 2500 # Number of validation examples considered\n",
    "vocab_size = 10000 # Number of words to be considered regarding on their frequency\n",
    "max_length = 400 # Maximum length of a sentence --> Size of entrance of neural network\n",
    "embedding_dim = 32 # Embedding dimension for vector representation of words\n",
    "nb_epochs = 20 # Number of epochs for training\n",
    "batch_size = 256 # Number training examples in mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training JSON file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = []\n",
    "URL = []\n",
    "X = [] # sentences\n",
    "Y = [] # sarcastic or not\n",
    "\n",
    "for line in open('review_amazon_ordinateurportable.json', 'r'):\n",
    "    training_examples.append(json.loads(line))\n",
    "\n",
    "training_examples = training_examples[0]\n",
    "\n",
    "for item in training_examples:\n",
    "    X.append(item['review'])    \n",
    "    Y.append(item['positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Training / Test dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_examples = list(zip(X, Y))\n",
    "random.shuffle(training_examples)\n",
    "X, Y = zip(*training_examples)\n",
    "\n",
    "X_train = X[0:nb_train]\n",
    "Y_train = Y[0:nb_train]\n",
    "\n",
    "X_valid = X[nb_train:nb_train + nb_valid]\n",
    "Y_valid = Y[nb_train:nb_train + nb_valid]\n",
    "\n",
    "X_test = X[nb_train + nb_valid:]\n",
    "Y_test = Y[nb_train + nb_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Sentences tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = vocab_size) # Strategy for considering Out Of Vocabulary words\n",
    "tokenizer.fit_on_texts(X_train) # Only considering words in training examples\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pre-processing training set:\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "# Pre-processing validation set:\n",
    "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "X_valid = pad_sequences(X_valid, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "# Pre-processing test set:\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Model architecture definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100, return_sequences = True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
    "    tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 400, 200)          106400    \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                4824      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 672,049\n",
      "Trainable params: 672,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_list = []\n",
    "\n",
    "callback_list.append(EarlyStopping(monitor = 'val_loss', patience = 15, min_delta = 0.0, mode = 'min')) # Callback to ensure parameters update in accordance with test accuracy improvement\n",
    "callback_list.append(CSVLogger('training_log.csv')) # Callback to get a feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 - 807s - loss: 0.6692 - accuracy: 0.5777 - val_loss: 0.6010 - val_accuracy: 0.6904\n",
      "Epoch 2/20\n",
      "40/40 - 847s - loss: 0.5359 - accuracy: 0.7413 - val_loss: 0.5486 - val_accuracy: 0.7276\n",
      "Epoch 3/20\n",
      "40/40 - 874s - loss: 0.4590 - accuracy: 0.7874 - val_loss: 0.5164 - val_accuracy: 0.7280\n",
      "Epoch 4/20\n",
      "40/40 - 932s - loss: 0.4009 - accuracy: 0.7979 - val_loss: 0.5112 - val_accuracy: 0.7376\n",
      "Epoch 5/20\n",
      "40/40 - 1153s - loss: 0.3537 - accuracy: 0.8148 - val_loss: 0.4805 - val_accuracy: 0.7548\n",
      "Epoch 6/20\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "X_valid = np.array(X_valid)\n",
    "Y_valid = np.array(Y_valid)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs = nb_epochs, batch_size = batch_size, validation_data = (X_valid, Y_valid), verbose = 2, callbacks = callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Model training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Accuracy & Loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n",
    "    \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Loading log file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.read_csv('training_log.csv')\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Predictions on test set & new sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_prediction = np.round_(model.predict(X_test)[:,0])\n",
    "conf_matrix = confusion_matrix(Y_test_prediction, Y_test)\n",
    "\n",
    "print('CONFUSION MATRIX' + '\\n' + '='*20)\n",
    "print('TRUE POSITIVES : ' + str(conf_matrix[0][0]) + '\\nFALSE POSITIVES : ' + str(conf_matrix[0][1]) + '\\nFALSE NEGATIVES : ' + str(conf_matrix[1][0]) + '\\nTRUE NEGATIVES : ' + str(conf_matrix[1][1]))\n",
    "print('-'*20 + '\\nACCURACY : ' + str(np.round_(100 * (conf_matrix[0][0] + conf_matrix[1][1])/(conf_matrix[0][0] + conf_matrix[1][0] + conf_matrix[0][1] + conf_matrix[1][1]),2)) + ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"granny starting to fear spiders in the garden might be real\"]\n",
    "sequence = tokenizer.texts_to_sequences(sentence)\n",
    "sentence = pad_sequences(sequence, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "print(np.round_(model.predict(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Model parameters save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"seq_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - Saved model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('seq_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
